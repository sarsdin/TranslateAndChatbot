{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a79e1bc4",
   "metadata": {},
   "source": [
    "# 모델 생성 및 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cba395a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T07:43:43.634512Z",
     "start_time": "2023-01-26T07:43:43.612117Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA 사용 가능 여부 : True\n",
      "현재 사용 device : cuda:0\n",
      "CUDA Index : 0\n",
      "GPU 이름 : NVIDIA GeForce RTX 3090\n",
      "GPU 개수 : 1\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import gc\n",
    "import time\n",
    "\n",
    "from urllib import response\n",
    "import requests\n",
    "import json\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device('cuda:0' if USE_CUDA else 'cpu')\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "print('CUDA 사용 가능 여부 :', USE_CUDA)\n",
    "print('현재 사용 device :', device)\n",
    "print('CUDA Index :', torch.cuda.current_device())\n",
    "print('GPU 이름 :', torch.cuda.get_device_name())\n",
    "print('GPU 개수 :', torch.cuda.device_count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b904a74",
   "metadata": {},
   "source": [
    "## 모델1 3.8b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d62a1f7",
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/polyglot-ko-3.8b\",\n",
    "                                          bos_token='[BOS]', eos_token='[EOS]', unk_token='[UNK]', pad_token='[PAD]',\n",
    "                                          mask_token='[MASK]'\n",
    "                                          )\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"EleutherAI/polyglot-ko-3.8b\", torch_dtype='auto', low_cpu_mem_usage=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee84bf10",
   "metadata": {},
   "source": [
    "## 모델2 5.8b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4af5be35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T07:45:17.523475Z",
     "start_time": "2023-01-26T07:43:47.360309Z"
    },
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/polyglot-ko-5.8b\",\n",
    "                                              bos_token='[BOS]', eos_token='[EOS]', unk_token='[UNK]', pad_token='[PAD]',\n",
    "                                              mask_token='[MASK]'\n",
    "                                              )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"EleutherAI/polyglot-ko-5.8b\", torch_dtype='auto', low_cpu_mem_usage=True).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6400f831",
   "metadata": {},
   "source": [
    "## 실행 메소드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055dcf3b",
   "metadata": {
    "code_folding": [],
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "prompt = \"GPTNeoX20B is a 20B-parameter autoregressive Transformer model developed by EleutherAI.\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "# input_ids = tokenizer(prompt, return_tensors=\"pt\").to(device).input_ids\n",
    "# .input_ids는 리턴되는 객체(=튜플)의 요소중 하나.\n",
    "# inputs_ids = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "model = model.to(device)\n",
    "outputs = model(inputs_ids)\n",
    "prediction_logits = outputs.logits\n",
    "\n",
    "# 문장 생성 부분\n",
    "gen_tokens = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    temperature=0.9,\n",
    "    max_length=150,\n",
    ")\n",
    "# 생성한 토큰들로 일괄적으로 생성하기 위해 배치메소드 이용\n",
    "gen_text = tokenizer.batch_decode(gen_tokens)[0]\n",
    "print(gen_text)\n",
    "\n",
    "# print(f\"gen_text: {inputs_ids['input_ids']}\")\n",
    "# print(f\"gen_text: {inputs_ids}\")\n",
    "# print(f\"gen_text: {prediction_logits}\")\n",
    "# print(f\"gen_text: {outputs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d4e207",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-25T10:09:23.650559Z",
     "start_time": "2023-01-25T10:09:15.823436Z"
    }
   },
   "outputs": [],
   "source": [
    "text = pipe(prompt, do_sample=True,  pad_token_id=50256,\n",
    "            num_return_sequences=1, top_k=40, top_p=0.9, max_new_tokens=150\n",
    "            )\n",
    "# num_beams=4, no_repeat_ngram_size=2, temperature=0.75, early_stopping=True,\n",
    "# max_length=50,\n",
    "print(text[0]['generated_text'])\n",
    "time.sleep(0.9)\n",
    "end = time.time()\n",
    "print(\"Time consumed in working: \", end - start)\n",
    "print(\"\\n\")\n",
    "\n",
    "text = pipe(\"아인슈타인은\", do_sample=True, pad_token_id=50255, max_new_tokens=140,\n",
    "            num_return_sequences=1, top_k=50, top_p=0.95\n",
    "            )\n",
    "print(text[0]['generated_text'])\n",
    "time.sleep(0.9)\n",
    "end = time.time()\n",
    "print(\"Time consumed in working: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "90fa72e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T08:17:03.041931Z",
     "start_time": "2023-01-26T08:17:03.034971Z"
    }
   },
   "outputs": [],
   "source": [
    "# model.generate()코드들을 pipeline()메소드로 축약시킨 코드가 가능\n",
    "pipe = pipeline(\n",
    "    'text-generation',\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0,\n",
    "    framework='pt'\n",
    ")\n",
    "\n",
    "# Only recent response text will return\n",
    "def processing_response(response_text: str ) -> str:\n",
    "    ret_text = \"\"\n",
    "    for i in range(0, len(response_text)):\n",
    "        if response_text[i] == \"\\n\" or response_text[i:i + 5] == \"설정목: \" or response_text[i:i + 4] == \"AI: \":\n",
    "            #print(response_text[i])\n",
    "            break\n",
    "        ret_text += response_text[i]\n",
    "    return ret_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5ddad168",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T08:17:36.083774Z",
     "start_time": "2023-01-26T08:17:33.547500Z"
    }
   },
   "outputs": [],
   "source": [
    "# The following is a conversation with an AI assistant. The assistant is helpful, creative, clever, and very friendly.\n",
    "prompt = \"\"\"\n",
    "다음 문장들은 인공지능과의 대화문이다. 인공지능의 성격은 희망적, 창조적이며 영리하고 친근하다.\n",
    "\n",
    "설정목: 오늘 날씨 어때? \n",
    "AI: 좋습니다. 당신의 하루는 오늘 어땠나요? \n",
    "설정목: 난 오늘 꽤 괜찮았지~ \n",
    "AI: 그렇군요! 모르는 것 있으면 물어보세요. 친절히 알려드리겠습니다.\n",
    "설정목: 겨울이긴한데... 날씨가 왜 이렇게 추워지는 건지 모르겠네? \n",
    "AI:\"\"\"\n",
    "\n",
    "text = pipe(\n",
    "    prompt,\n",
    "    do_sample=True,\n",
    "#     temperature=0.9,\n",
    "    top_p=0.9,\n",
    "    top_k=50,\n",
    "    max_new_tokens=50,\n",
    "#     repetition_penalty=0.8,\n",
    "    pad_token_id= 53456\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adf7fd5",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# url = \"https://eleuther-ai-gpt-j-6b-float16-text-generation-api-ainize-team.endpoint.ainize.ai/predictions/text-generation\"\n",
    "\n",
    "# base_prompt = get_base_prompt()\n",
    "base_prompt = prompt1\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "# End chat when user types \"exit\"\n",
    "while True:\n",
    "    chat = input(\"사람: \")\n",
    "    if chat == \"exit\":\n",
    "        break\n",
    "    prompt = f\"{base_prompt}\\n사람: {chat}\\n인공지능:\"\n",
    "\n",
    "    # data == string type dumps()는 json객체를 json문자열로 변환시켜줌.\n",
    "    data = json.dumps({\n",
    "        \"text_inputs\":prompt,\n",
    "        \"temperature\":0.9,\n",
    "        \"top_p\":0.95,\n",
    "        \"repetition_penalty\":0.8,\n",
    "        \"do_sample\":True,\n",
    "        \"top_k\":50,\n",
    "        \"length\":50,\n",
    "    })\n",
    "    # loads()는 json문자열을 json객체로 변환시켜줌.\n",
    "    jo = json.loads(data)\n",
    "    # text = pipe(\n",
    "    #     jo['text_inputs'],\n",
    "    #     do_sample=jo['do_sample'],\n",
    "    #     temperature=jo['temperature'],\n",
    "    #     top_p=jo['top_p'],\n",
    "    #     top_k=jo['top_k'],\n",
    "    #     max_new_tokens=jo['length'],\n",
    "    #     repetition_penalty=jo['repetition_penalty'],\n",
    "    #     pad_token_id=50256\n",
    "    # )\n",
    "    text = pipe(\n",
    "        prompt,\n",
    "        do_sample=True,\n",
    "        temperature=0.9,\n",
    "        top_p=0.95,\n",
    "        top_k=50,\n",
    "        max_new_tokens=40,\n",
    "        repetition_penalty=0.8,\n",
    "        pad_token_id=50256\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "857b108a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T08:17:38.744565Z",
     "start_time": "2023-01-26T08:17:38.740681Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "다음 문장들은 인공지능과의 대화문이다. 인공지능의 성격은 희망적, 창조적이며 영리하고 친근하다.\n",
      "\n",
      "설정목: 오늘 날씨 어때? \n",
      "AI: 좋습니다. 당신의 하루는 오늘 어땠나요? \n",
      "설정목: 난 오늘 꽤 괜찮았지~ \n",
      "AI: 그렇군요! 모르는 것 있으면 물어보세요. 친절히 알려드리겠습니다.\n",
      "설정목: 겨울이긴한데... 날씨가 왜 이렇게 추워지는 건지 모르겠네? \n",
      "AI: 겨울이라서 그런 것입니다. 노라고설정목: 그럼 겨울에 뭐해? 노라고AI: 전 겨울에는 주로 책을 읽거나 집에서 뒹굴뒹굴~ 노라고설정목: 겨울에 뭐할건데? \n",
      "\n",
      "\n",
      "겨울이라서 그런 것입니다. 노라고\n"
     ]
    }
   ],
   "source": [
    "    # res = requests.post(url, headers=headers, data=data)\n",
    "    # if res.status_code == 200:\n",
    "    # response_text = res.json()[0] # response에서 json응답일경우 바로 딕셔너리로 변환하고 0이라는 키의 값을 뽑아냄\n",
    "    print(text[0]['generated_text'])\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    response_text = text[0]['generated_text']\n",
    "    processing_text = processing_response(response_text[len(prompt):])\n",
    "    prompt += processing_text\n",
    "    print(processing_text)\n",
    "    #print(prompt)\n",
    "    #print(\"AI:\", processing_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5fcd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "\n",
    "# 끝\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165.545px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 256.386454,
   "position": {
    "height": "277.841px",
    "left": "1396.17px",
    "right": "20px",
    "top": "112px",
    "width": "280.591px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
